from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime
import pandas as pd


def init_driver():
    options = Options()
    # options.add_argument("--headless=new")  # UI ì—†ì´ ì‹¤í–‰í•  ë•Œë§Œ ì£¼ì„ í•´ì œ
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("lang=ko_KR")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)


def crawl_google_news_selenium(keyword, max_results=10):
    driver = init_driver()
    url = f"https://www.google.com/search?q={keyword}+ì¬ê°œë°œ&hl=ko&tbm=nws"
    driver.get(url)

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.SoaBEf"))
        )
    except:
        print("âŒ êµ¬ê¸€ ë‰´ìŠ¤ ë¡œë”© ì‹¤íŒ¨ (ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨)")
        driver.quit()
        return pd.DataFrame()

    articles = driver.find_elements(By.CSS_SELECTOR, "div.SoaBEf")
    print(f"ğŸ” êµ¬ê¸€ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜: {len(articles)}")
    data = []

    for article in articles[:max_results]:
        try:
            title_tag = article.find_element(By.CSS_SELECTOR, 'div[role="heading"]')
            title = title_tag.text.strip()
            link = article.find_element(By.TAG_NAME, "a").get_attribute("href")
            snippet = article.find_element(By.CSS_SELECTOR, "div:nth-child(3)").text.strip()
            data.append({
                "ì œëª©": title,
                "ë§í¬": link,
                "ìš”ì•½": snippet,
                "ì •ë³´_ì›ì²œ": "Google",
                "í¬ë¡¤ë§_ì‹œê°„": datetime.now()
            })
        except Exception as e:
            print("âŒ êµ¬ê¸€ ë‰´ìŠ¤ íŒŒì‹± ì‹¤íŒ¨:", e)
            continue

    driver.quit()
    return pd.DataFrame(data)


def crawl_naver_news_selenium(keyword, max_results=10):
    driver = init_driver()
    url = f"https://search.naver.com/search.naver?where=news&query={keyword}+ì¬ê°œë°œ"
    driver.get(url)

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located(
                (By.CSS_SELECTOR, "span.sds-comps-text-type-headline1"))
        )
    except:
        print("âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ë¡œë”© ì‹¤íŒ¨ (ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨)")
        driver.quit()
        return pd.DataFrame()

    titles = driver.find_elements(By.CSS_SELECTOR, "span.sds-comps-text-type-headline1")
    bodies = driver.find_elements(By.CSS_SELECTOR, "span.sds-comps-text-type-body1")
    links = driver.find_elements(By.CSS_SELECTOR, "a[href^='https://']")

    print(f"ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ì œëª© ìˆ˜: {len(titles)}")

    data = []
    for i in range(min(max_results, len(titles))):
        try:
            title = titles[i].text.strip()
            snippet = bodies[i].text.strip() if i < len(bodies) else ""
            link = links[i].get_attribute("href")
            data.append({
                "ì œëª©": title,
                "ë§í¬": link,
                "ìš”ì•½": snippet,
                "ì •ë³´_ì›ì²œ": "Naver",
                "í¬ë¡¤ë§_ì‹œê°„": datetime.now()
            })
        except Exception as e:
            print("âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ íŒŒì‹± ì‹¤íŒ¨:", e)
            continue

    driver.quit()
    return pd.DataFrame(data)
